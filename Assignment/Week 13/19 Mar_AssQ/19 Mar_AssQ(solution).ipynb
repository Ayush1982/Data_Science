{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as normalization, is a technique used in data preprocessing to scale features so that they lie within a specific range, typically between 0 and 1. This scaling method is beneficial when the features have different scales and ranges, and you want to bring them to a standardized range to prevent features with larger values from dominating those with smaller values during modeling.\n",
    "\n",
    "**Usage:**\n",
    "   Min-Max scaling is applied to each feature independently, ensuring that each feature's range is transformed to the desired range (usually [0, 1]).\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider a dataset with a single feature representing house prices, with values ranging from $100,000 to $1,000,000. We'll perform Min-Max scaling to bring these values into the range [0, 1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.        ]\n",
      " [0.11111111]\n",
      " [0.44444444]\n",
      " [0.77777778]\n",
      " [1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[100000],\n",
    "                 [200000],\n",
    "                 [500000],\n",
    "                 [800000],\n",
    "                 [1000000]])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Unit Vector Technique:**\n",
    "   - Each feature vector is scaled independently to have unit norm (length 1).\n",
    "   - It preserves the direction of the data.\n",
    "   - The magnitude of the features may vary depending on the data distribution.\n",
    "   - It's useful for algorithms that rely on the direction of the data vectors rather than their magnitude, such as PCA (Principal Component Analysis) or clustering algorithms like K-means.\n",
    "\n",
    "2. **Min-Max Scaling:**\n",
    "   - Scales each feature independently to a fixed range (e.g., [0, 1] or [-1, 1]).\n",
    "   - It preserves the distribution of the data and the relationships between different features.\n",
    "   - The magnitude of the features is explicitly controlled and bounded.\n",
    "   - It's suitable for algorithms like neural networks and SVMs (Support Vector Machines) that require input data to be on the same scale.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let's consider a dataset with two features: `age` (ranging from 20 to 60 years) and `income` (ranging from 20,000 to 100,000 USD). We'll apply both Unit Vector scaling and Min-Max scaling to this dataset.\n",
    "\n",
    "**Unit Vector Scaling:**\n",
    "\n",
    "After applying Unit Vector scaling, the data will be scaled such that each sample has a length of 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.21081851 0.13483997]\n",
      " [0.31622777 0.26967994]\n",
      " [0.42163702 0.40451992]\n",
      " [0.52704628 0.53935989]\n",
      " [0.63245553 0.67419986]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "data = np.array([[20, 20000],\n",
    "                 [30, 40000],\n",
    "                 [40, 60000],\n",
    "                 [50, 80000],\n",
    "                 [60, 100000]])\n",
    "\n",
    "scaled_data_unit_vector = normalize(data, axis=0)\n",
    "print(scaled_data_unit_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Min-Max Scaling:**\n",
    "\n",
    "After applying Min-Max scaling, each feature will be scaled to a range between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.  ]\n",
      " [0.25 0.25]\n",
      " [0.5  0.5 ]\n",
      " [0.75 0.75]\n",
      " [1.   1.  ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[20, 20000],\n",
    "                 [30, 40000],\n",
    "                 [40, 60000],\n",
    "                 [50, 80000],\n",
    "                 [60, 100000]])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data_min_max = scaler.fit_transform(data)\n",
    "print(scaled_data_min_max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction in data analysis and machine learning. It aims to reduce the number of variables (or features) in a dataset while preserving as much information as possible. PCA achieves this by transforming the original variables into a new set of variables, called principal components, which are linear combinations of the original variables.\n",
    "\n",
    "PCA working:\n",
    "\n",
    "1. **Calculate Covariance Matrix:**\n",
    "   PCA starts by calculating the covariance matrix of the original dataset. The covariance matrix describes the relationships between different variables in the dataset.\n",
    "\n",
    "2. **Eigenvalue Decomposition:**\n",
    "   Next, PCA performs eigenvalue decomposition on the covariance matrix to obtain eigenvectors and eigenvalues. Eigenvectors represent the directions (or principal components) of maximum variance in the data, while eigenvalues represent the magnitude of variance along those directions.\n",
    "\n",
    "3. **Select Principal Components:**\n",
    "   PCA sorts the eigenvectors based on their corresponding eigenvalues in descending order. The eigenvectors with the highest eigenvalues (i.e., the principal components with the most variance) are selected as the new basis for the transformed dataset.\n",
    "\n",
    "4. **Transform Data:**\n",
    "   Finally, PCA projects the original dataset onto the subspace spanned by the selected principal components, effectively reducing the dimensionality of the data while retaining most of its variance.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let's consider a dataset with three features: height, weight, and age of individuals. We'll perform PCA to reduce the dimensionality of the dataset from three dimensions to two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.81682226   1.16417226]\n",
      " [ 15.70466602  -0.65183117]\n",
      " [-11.21696276  -1.11090273]\n",
      " [ -5.30452552   0.59856164]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'Height (cm)': [170, 160, 180, 175],\n",
    "    'Weight (kg)': [65, 55, 70, 68],\n",
    "    'Age (years)': [30, 25, 35, 32]\n",
    "})\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "transformed_data = pca.fit_transform(data)\n",
    "print(transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) and feature extraction are closely related concepts in the field of machine learning and data analysis. PCA can be used as a technique for feature extraction, where it transforms the original features into a new set of features (principal components) that capture the most important information or variation in the data.\n",
    "\n",
    "1. **Dimensionality Reduction:**\n",
    "   PCA is primarily used for dimensionality reduction by transforming a high-dimensional dataset into a lower-dimensional subspace while retaining most of the variability in the data. In this process, PCA identifies the directions (principal components) along which the data varies the most and projects the data onto these components.\n",
    "\n",
    "2. **Feature Extraction:**\n",
    "   Each principal component in PCA is a linear combination of the original features, where the coefficients of this combination represent the importance of each original feature in the principal component. Therefore, PCA effectively extracts new features that are linear combinations of the original features, with each new feature capturing a different aspect of the variability in the data.\n",
    "\n",
    "3. **Reduced Dimensionality Features:**\n",
    "   The principal components obtained through PCA serve as reduced dimensionality features that can be used in place of the original features for various machine learning tasks. These new features are often orthogonal (uncorrelated) to each other, which can be advantageous for certain algorithms.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider a dataset with three features representing the dimensions of geometric shapes: length, width, and height. We'll use PCA to perform feature extraction and reduce the dimensionality of the dataset from three dimensions to two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.64922922  0.28018104]\n",
      " [ 2.3439235  -0.07760566]\n",
      " [-2.3439235   0.07760566]\n",
      " [-0.64922922 -0.28018104]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'Length': [5, 4, 7, 6],\n",
    "    'Width': [3, 2, 5, 4],\n",
    "    'Height': [2, 1, 3, 2]\n",
    "})\n",
    "pca = PCA(n_components=2)\n",
    "transformed_data = pca.fit_transform(data)\n",
    "\n",
    "print(transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Understand the Dataset:**\n",
    "   Start by understanding the dataset and its features. In this case, the dataset contains features such as price, rating, and delivery time, which are likely on different scales.\n",
    "\n",
    "2. **Perform Min-Max Scaling:**\n",
    "   Min-Max scaling will be applied to each feature independently to bring their values within a specific range, typically between 0 and 1. This ensures that all features contribute equally to the recommendation system, regardless of their original scale.\n",
    "\n",
    "4. **Implement Min-Max Scaling:**\n",
    "   You can use built-in functions or libraries in your programming language to implement Min-Max scaling. Many machine learning libraries, such as scikit-learn in Python, provide utilities for scaling data.\n",
    "\n",
    "5. **Apply to the Entire Dataset:**\n",
    "   Apply Min-Max scaling to the entire dataset, including all features (price, rating, delivery time).\n",
    "\n",
    "6. **Validate and Evaluate:**\n",
    "   After preprocessing the data, validate the scaled dataset to ensure that all values fall within the desired range (typically [0, 1]). Evaluate the impact of scaling on the performance of your recommendation system, and adjust as necessary.\n",
    "\n",
    "7. **Build Recommendation System:**\n",
    "   With the preprocessed data, we can proceed to build the recommendation system using techniques such as collaborative filtering, content-based filtering, or hybrid methods, depending on the specific requirements of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Understand the Dataset:**\n",
    "   Begin by understanding the dataset, including the various features related to company financial data (e.g., revenue, profit, debt, etc.) and market trends (e.g., stock market indices, economic indicators, etc.).\n",
    "\n",
    "2. **Normalize the Data:**\n",
    "   Before applying PCA, it's essential to normalize or standardize the data to ensure that all features are on the same scale. This step is crucial because PCA is sensitive to the scale of the features.\n",
    "\n",
    "3. **Apply PCA:**\n",
    "   Once the data is normalized, apply PCA to the dataset. PCA will identify the principal components, which are linear combinations of the original features that capture the most significant variability in the data. The number of principal components to retain can be determined based on the desired level of variance explained or by using techniques such as the explained variance ratio.\n",
    "\n",
    "4. **Select the Number of Components:**\n",
    "   Determine the number of principal components to retain based on the explained variance ratio. You can plot the cumulative explained variance ratio versus the number of components and choose the number of components that capture a significant portion of the total variance in the data (e.g., 95% or 99%).\n",
    "\n",
    "5. **Transform the Data:**\n",
    "   Transform the original dataset into the lower-dimensional subspace spanned by the selected principal components. This transformation reduces the dimensionality of the dataset while retaining most of the variability in the data.\n",
    "\n",
    "6. **Model Building:**\n",
    "   Use the transformed dataset with reduced dimensionality for building the predictive model to predict stock prices. You can use various machine learning algorithms such as regression models, time series models, or deep learning models, depending on the nature of the problem and the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform Min-Max scaling to transform the given values to a range of -1 to 1, you can follow these steps:\n",
    "\n",
    "1. Calculate the minimum and maximum values in the dataset.\n",
    "2. Apply the Min-Max scaling formula to each value in the dataset.\n",
    "3. Scale the values to the desired range.\n",
    "\n",
    "Let's calculate:\n",
    "\n",
    "1. **Minimum value (min_val) = 1**\n",
    "2. **Maximum value (max_val) = 20**\n",
    "3. **Desired range: -1 to 1**\n",
    "\n",
    "calculate the scaled values:\n",
    "\n",
    "x_1 = {(1 - 1)  *  1 - (-1) / (20 - 1)} + (-1) = -1 \n",
    "x_2 = {(5 - 1)  *  1 - (-1) / (20 - 1)} + (-1) = -0.6\n",
    "x_3 = {(10 - 1) *  1 - (-1) / (20 - 1)} + (-1) = 0 \n",
    "x_4 = {(15 - 1) *  1 - (-1) / (20 - 1)} + (-1) = 0.6\n",
    "x_5 = {(20 - 1) *  1 - (-1) / (20 - 1)} + (-1) = 1 \n",
    "\n",
    "So, the Min-Max scaled values for the dataset [1, 5, 10, 15, 20] in the range of -1 to 1 are:\n",
    "\n",
    "[-1, -0.6, 0, 0.6, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform feature extraction using PCA in Python for the given dataset containing features like height, weight, age, gender, and blood pressure, we first need to follow these steps:\n",
    "\n",
    "1. Normalize the data.\n",
    "2. Apply PCA.\n",
    "3. Determine the number of principal components to retain based on the explained variance ratio.\n",
    "4. Choose the number of principal components to retain based on the desired level of variance explained or based on specific requirements.\n",
    "\n",
    "Here's how you can implement this in Python:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.49730342   0.74676823]\n",
      " [ -0.4808717   13.85303127]\n",
      " [-12.38208912  -6.26471423]\n",
      " [ -4.45099323  -3.23910479]\n",
      " [ 16.81665062  -5.09598047]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'height': [170, 165, 180, 175, 160],\n",
    "    'weight': [65, 70, 75, 60, 55],\n",
    "    'age': [30, 25, 35, 40, 28],\n",
    "    'gender': [1 , 1 , 0 , 0 , 1],                 # male = 1, female = 0\n",
    "    'blood_pressure': [120, 130, 115, 125, 110]\n",
    "})\n",
    "\n",
    "pca = PCA(n_components=2 )\n",
    "transformed_data = pca.fit_transform(data)\n",
    "print(transformed_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
