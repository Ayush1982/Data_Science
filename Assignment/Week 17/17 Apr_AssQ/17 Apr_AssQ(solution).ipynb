{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique used for regression tasks, where the goal is to predict a continuous numerical value. It is an ensemble learning method that combines the predictions from multiple individual models, typically decision trees, to create a stronger predictive model.\n",
    "\n",
    "Here's how Gradient Boosting Regression works:\n",
    "\n",
    "1. **Decision Trees**: The base learners, often decision trees, are constructed sequentially. Each tree attempts to correct the errors made by the previous ones. Initially, a simple model (usually a single leaf) is used as the first approximation.\n",
    "\n",
    "2. **Loss Function Optimization**: A loss function, typically a measure of the difference between the actual and predicted values, is defined. In regression tasks, common loss functions include mean squared error (MSE) or mean absolute error (MAE). The objective is to minimize this loss function.\n",
    "\n",
    "3. **Gradient Descent**: Gradient descent is used to minimize the loss function. In each iteration, the algorithm calculates the gradient of the loss function with respect to the prediction, and then adjusts the prediction in the direction that minimizes the loss. This adjustment is made by fitting a new model (tree) to the residuals (the differences between actual and predicted values).\n",
    "\n",
    "4. **Combining Models**: The predictions from all the individual models (trees) are combined to make the final prediction. This is typically done by summing up the predictions from all the trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 0.0041597011570905906\n",
      "Test MSE: 1.3379888778506104\n",
      "Train R-squared: 0.9999970841396733\n",
      "Test R-squared: 0.9990403356176427\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.base_prediction = np.mean(y)\n",
    "        y_pred = np.full_like(y, self.base_prediction)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = y - y_pred\n",
    "            tree = self._fit_tree(X, residuals)\n",
    "            tree_pred = tree.predict(X)\n",
    "            y_pred += self.learning_rate * tree_pred\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def _fit_tree(self, X, residuals):\n",
    "        tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "        tree.fit(X, residuals)\n",
    "        return tree\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.full(X.shape[0], self.base_prediction)\n",
    "        for tree in self.trees:\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    return 1 - (ss_res / ss_tot)\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = gb_regressor.predict(X_train)\n",
    "y_pred_test = gb_regressor.predict(X_test)\n",
    "\n",
    "print(\"Train MSE:\", mean_squared_error(y_train, y_pred_train))\n",
    "print(\"Test MSE:\", mean_squared_error(y_test, y_pred_test))\n",
    "print(\"Train R-squared:\", r_squared(y_train, y_pred_train))\n",
    "print(\"Test R-squared:\", r_squared(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "param_grid = {\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=gb_regressor, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "best_gb_model = grid_search.best_estimator_\n",
    "y_pred = best_gb_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Gradient Boosting, a weak learner refers to a simple model that performs slightly better than random guessing on a given task. Weak learners are typically used as base estimators in ensemble methods like Gradient Boosting. In the context of Gradient Boosting, decision trees are often employed as weak learners, although other types of models can also be used.\n",
    "\n",
    "Here are some characteristics of weak learners in the context of Gradient Boosting:\n",
    "\n",
    "1. **Simple Model**: Weak learners are intentionally kept simple to prevent overfitting and to ensure computational efficiency. In the case of decision trees, these might be trees with very few nodes (e.g., stumps or trees with one or two levels).\n",
    "\n",
    "2. **Limited Predictive Power**: Weak learners have limited predictive power on their own and may perform only slightly better than random guessing. However, they are still capable of capturing some patterns or relationships within the data.\n",
    "\n",
    "3. **Sequential Improvement**: In Gradient Boosting, weak learners are sequentially added to the ensemble, and each subsequent learner focuses on reducing the errors that were not effectively captured by the previous ones. This sequential approach allows the ensemble to gradually improve its predictive performance.\n",
    "\n",
    "4. **Combined Strength**: Although weak learners individually may not be very accurate, their collective predictions, when appropriately combined through boosting, result in a strong learner with high predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Sequential Learning**: Gradient Boosting builds an ensemble of weak learners sequentially, where each learner tries to correct the errors made by the combination of previous learners. This sequential approach allows the algorithm to iteratively improve its predictions.\n",
    "\n",
    "2. **Gradient Descent Optimization**: At each iteration, Gradient Boosting fits a weak learner to the residuals (the differences between the actual and predicted values) of the previous iteration. It does this by minimizing a loss function using gradient descent. This means that each new weak learner is trained to capture the patterns or relationships that the previous weak learners failed to capture.\n",
    "\n",
    "3. **Gradient of the Loss Function**: The algorithm calculates the gradient of the loss function with respect to the predictions of the current ensemble. This gradient provides information about the direction and magnitude of the error, guiding the algorithm to update the predictions in a way that reduces the loss.\n",
    "\n",
    "4. **Combining Weak Learners**: The predictions from all the weak learners are combined to make the final prediction. Typically, this combination involves summing up the predictions from all the learners, weighted by a learning rate parameter. This allows the algorithm to control the contribution of each weak learner to the final prediction.\n",
    "\n",
    "5. **Regularization**: Gradient Boosting includes regularization techniques to prevent overfitting, such as controlling the complexity of the weak learners (e.g., limiting the depth of decision trees) and using shrinkage (learning rate) to slow down the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners through a sequential process. Here's a step-by-step explanation of how this ensemble is constructed:\n",
    "\n",
    "1. **Initialize the Ensemble**: Gradient Boosting starts by initializing the ensemble with a simple model, often a constant value (e.g., the mean of the target variable for regression tasks, or the most frequent class for classification tasks). This initial model serves as the starting point for subsequent iterations.\n",
    "\n",
    "2. **Compute Pseudo-Residuals**: For each training example, compute the pseudo-residuals, which are the negative gradients of the loss function with respect to the current predictions. These pseudo-residuals represent the direction and magnitude of the error that the ensemble has made on each training example.\n",
    "\n",
    "3. **Fit a Weak Learner to Pseudo-Residuals**: Train a weak learner (usually a decision tree) to predict the pseudo-residuals. This weak learner is fitted to the pseudo-residuals rather than the original target values. The goal is to capture the patterns or relationships in the data that were not adequately captured by the current ensemble.\n",
    "\n",
    "4. **Update Ensemble Predictions**: Combine the predictions of the new weak learner with the predictions of the current ensemble. The combination involves adding a fraction (learning rate) of the predictions from the new weak learner to the predictions of the current ensemble. This update is done to gradually improve the predictions of the ensemble.\n",
    "\n",
    "5. **Iterative Process**: Repeat steps 2-4 for a predefined number of iterations or until a stopping criterion is met. In each iteration, the algorithm focuses on reducing the errors (pseudo-residuals) made by the current ensemble, iteratively improving its predictive performance.\n",
    "\n",
    "6. **Final Prediction**: After all iterations are completed, the final prediction of the ensemble is obtained by summing up the predictions from all the weak learners. Optionally, a shrinkage parameter (learning rate) can be applied to control the contribution of each weak learner to the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing the mathematical intuition behind the Gradient Boosting algorithm involves understanding several key components and steps. Here's a breakdown of the main steps involved:\n",
    "\n",
    "1. **Loss Function**: Define a differentiable loss function L(y,F(x))L(y,F(x)), where yy represents the true target values and F(x)F(x) represents the current ensemble's predictions for input xx. Common loss functions include mean squared error (MSE) for regression and log loss (or cross-entropy loss) for classification.\n",
    "\n",
    "2. **Initial Prediction**: Initialize the ensemble's prediction as a constant value, often the mean of the target values for regression tasks or the log odds of the class distribution for classification tasks.\n",
    "\n",
    "3. **Compute Pseudo-Residuals**: Calculate the negative gradient of the loss function with respect to the current ensemble's predictions for each training example.\n",
    "\n",
    "4. **Fit Weak Learner to Pseudo-Residuals**: Train a weak learner (e.g., decision tree) to predict the pseudo-residuals. The weak learner aims to capture the patterns in the data that the current ensemble fails to capture. This step minimizes the loss when applied to the pseudo-residuals rather than the original target values.\n",
    "\n",
    "5. **Update Ensemble Predictions**: Adjust the ensemble's predictions by adding a fraction of the predictions from the weak learner to the current ensemble's predictions. \n",
    "\n",
    "6. **Iterative Process**: Repeat steps 3-5 for a predefined number of iterations or until a stopping criterion is met. In each iteration, the weak learner focuses on reducing the errors made by the current ensemble, leading to gradual improvement in predictive performance.\n",
    "\n",
    "7. **Final Prediction**: After completing all iterations, the final prediction of the Gradient Boosting ensemble is obtained by summing up the predictions from all weak learners, weighted by the learning rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
