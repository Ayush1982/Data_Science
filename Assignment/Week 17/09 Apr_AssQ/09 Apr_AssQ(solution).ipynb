{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. What is Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes' theorem is a fundamental concept in probability theory and statistics named after the Reverend Thomas Bayes. It describes the probability of an event, based on prior knowledge of conditions that might be related to the event. Mathematically, it can be expressed as:\n",
    "\n",
    "P(A|B) = {P(B|A) * P(A)} / {P(B)}\n",
    "\n",
    "Where:\n",
    "- P(A|B) is the conditional probability of event A occurring given that event B has occurred.\n",
    "- P(B|A) is the conditional probability of event B occurring given that event A has occurred.\n",
    "- P(A) and P(B) are the probabilities of events A and B occurring independently of each other.\n",
    "\n",
    "In simpler terms, Bayes' theorem allows us to update our beliefs about the likelihood of an event happening based on new evidence. It's widely used in various fields, including statistics, machine learning, artificial intelligence, and medical diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. What is the formula for Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for Bayes' theorem is:\n",
    "\n",
    "P(A|B) = {P(B|A) * P(A)} / {P(B)}\n",
    "\n",
    "Where:\n",
    "- P(A|B) is the conditional probability of event A occurring given that event B has occurred.\n",
    "- P(B|A) is the conditional probability of event B occurring given that event A has occurred.\n",
    "- P(A) and P(B) are the probabilities of events A and B occurring independently of each other.\n",
    "\n",
    "\n",
    "This formula allows us to update our beliefs about the likelihood of an event happening based on new evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. How is Bayes' theorem used in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Medical Diagnosis**: Bayes' theorem is used in medical diagnosis to calculate the probability of a patient having a particular disease given certain symptoms. It helps doctors update their beliefs about the likelihood of a disease based on diagnostic tests and patient symptoms.\n",
    "\n",
    "2. **Spam Filtering**: In email spam filtering, Bayes' theorem is used to classify emails as spam or non-spam. It helps in updating the probability that an email is spam or non-spam based on the occurrence of certain words or features within the email.\n",
    "\n",
    "3. **Machine Learning**: Bayes' theorem is utilized in various machine learning algorithms, especially in Bayesian models such as Naive Bayes classifiers. These models make predictions based on probabilistic principles, updating beliefs about class membership given observed data.\n",
    "\n",
    "4. **Weather Forecasting**: Bayes' theorem is employed in weather forecasting to update the probability of weather conditions based on new information, such as satellite imagery, atmospheric data, and historical patterns.\n",
    "\n",
    "5. **Financial Modeling**: In finance, Bayes' theorem is applied in risk assessment, portfolio management, and forecasting. It helps investors update their beliefs about asset returns or market conditions based on new economic indicators or financial data.\n",
    "\n",
    "6. **Fault Diagnosis in Engineering**: Bayes' theorem is used in fault diagnosis and reliability engineering to estimate the probability of system failures or malfunctions given observed data from sensors or performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. What is the relationship between Bayes' theorem and conditional probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes' theorem is closely related to conditional probability. Conditional probability refers to the probability of an event occurring given that another event has already occurred. Bayes' theorem provides a mathematical formula for calculating conditional probabilities in situations where there is uncertainty or incomplete information.\n",
    "\n",
    "The relationship between Bayes' theorem and conditional probability is evident in the formula itself:\n",
    "\n",
    "P(A|B) = {P(B|A) * P(A)} / {P(B)}\n",
    "\n",
    "Bayes' theorem provides a way to update our beliefs about the probability of an event (A) occurring based on new evidence (event B). It allows us to revise our initial probabilities (prior probabilities) using observed data or information (likelihood) to obtain updated probabilities (posterior probabilities). In essence, Bayes' theorem helps us understand how to adjust our beliefs in light of new evidence, making it a fundamental concept in probability theory and statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Gaussian Naive Bayes**: This classifier assumes that the continuous features follow a Gaussian (normal) distribution. It's suitable for problems where the features are continuous and can be reasonably modeled by a Gaussian distribution, such as some types of sensor data or measurements.\n",
    "\n",
    "2. **Multinomial Naive Bayes**: This classifier is appropriate for problems with discrete features (e.g., word counts in text classification). It's commonly used in text classification tasks, such as spam detection or sentiment analysis.\n",
    "\n",
    "3. **Bernoulli Naive Bayes**: Similar to Multinomial Naive Bayes, this classifier is suitable for problems with binary or Boolean features. It's often used in text classification tasks where features represent the presence or absence of certain words.\n",
    "\n",
    "When deciding which type of Naive Bayes classifier to use, consider the following:\n",
    "\n",
    "- **Nature of Features**: Determine whether your features are continuous, discrete, or binary. Gaussian Naive Bayes is suitable for continuous features, while Multinomial and Bernoulli Naive Bayes are appropriate for discrete or binary features.\n",
    "\n",
    "- **Assumptions**: Understand the assumptions made by each type of Naive Bayes classifier. For example, Gaussian Naive Bayes assumes that features are normally distributed, while Multinomial and Bernoulli Naive Bayes assume features are independent and follow a multinomial or Bernoulli distribution, respectively.\n",
    "\n",
    "- **Size of Dataset**: Consider the size of your dataset and the sparsity of your feature space. Multinomial and Bernoulli Naive Bayes can handle large datasets with high-dimensional feature spaces efficiently, while Gaussian Naive Bayes may be less efficient for high-dimensional data.\n",
    "\n",
    "- **Performance**: Experiment with different types of Naive Bayes classifiers and evaluate their performance using cross-validation or other validation techniques. Choose the classifier that provides the best performance metrics for your specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6. Assignment:\n",
    "You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive \n",
    "Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of each feature value for each class:\n",
    "\n",
    "Class\t  X1=1     X1=2    \tX1=3    \tX2=1    \tX2=2    \tX2=3\t    X2=4\n",
    "A\t       3         3       4           4            3          3            3\n",
    "B \t       2         2       1           2            2          2            3\n",
    "\n",
    "Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance to belong to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the class of the new instance using Naive Bayes, we need to calculate the conditional probabilities of each class given the observed feature values X1=3X1​=3 and X2=4X2​=4, and then choose the class with the highest probability.\n",
    "\n",
    "Let's denote the classes A and B as CACA​ and CBCB​ respectively.\n",
    "\n",
    "Given that the prior probabilities are equal for each class (i.e., P(CA)=P(CB)=0.5P(CA​)=P(CB​)=0.5), we can calculate the conditional probabilities using \n",
    "\n",
    "For class A:\n",
    "P(X1=3,X2=4∣CA)×P(CA)​=2/10\n",
    "\n",
    "For class B:\n",
    "P(X1=3,X2=4∣CB)×P(CB)=3/22\n",
    "\n",
    "Comparing the numerators, we find that 2/10 > 3/22​, so Naive Bayes would predict the new instance to belong to class A.\n",
    "\n",
    "Therefore, the Naive Bayes classifier would predict the new instance to belong to class A."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
